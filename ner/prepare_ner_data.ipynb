{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.dataset_walker import DatasetWalker\n",
    "from scripts.knowledge_reader import KnowledgeReader\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BRIDGE GUEST HOUSE'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_walker = DatasetWalker('train', './data', labels=True)\n",
    "kng_reader = KnowledgeReader(dataroot='./data/')\n",
    "kng_reader.get_entity_name('hotel', 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "RE_PUNC = re.compile(r'[!\"#$%&()*+,-./:;<=>?@\\[\\]\\\\^`{|}~_\\']')\n",
    "def remove_punc(_text):\n",
    "    return RE_PUNC.sub(\" \", _text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "history_max_utterances = 400\n",
    "for m, (log, label) in enumerate(ds_walker):\n",
    "\n",
    "    #print(\"m\", m)\n",
    "\n",
    "    if(label['target'] == False):\n",
    "        continue\n",
    "\n",
    "    with open('./baseline/resources/entity_mapping.json', 'r') as fr:\n",
    "        all_entity_names_norm = json.load(fr)\n",
    "    \n",
    "    curr_history = \"\"\n",
    "    curr_sample = {}\n",
    "\n",
    "    for utter in log:\n",
    "        curr_history += utter['text'] + \" \"\n",
    "    \n",
    "    curr_history = remove_punc(curr_history)\n",
    "    curr_history = curr_history[-history_max_utterances:]\n",
    "\n",
    "    curr_sent_words = curr_history.split(\" \")\n",
    "    entity_name = kng_reader.get_entity_name(label['knowledge'][0]['domain'], label['knowledge'][0]['entity_id'])\n",
    "    #entity_name_words = entity_name.split(\" \")\n",
    "    entity_names = [entity_name] + all_entity_names_norm.get(entity_name.lower(), [])  ## get all the names in the entity\n",
    "\n",
    "    #print(entity_names)\n",
    "\n",
    "    label = [\"O\"]*len(curr_sent_words)\n",
    "\n",
    "    for i,word in enumerate(curr_sent_words):\n",
    "\n",
    "        for e_name in entity_names:\n",
    "            entity_name_words = e_name.split(\" \")\n",
    "            if (word.lower() == entity_name_words[0].lower()):\n",
    "                temp = i+1\n",
    "                flag = True\n",
    "                for j in entity_name_words[1:]:\n",
    "                    if (temp >= len(curr_sent_words) or curr_sent_words[temp].lower() != j.lower()):\n",
    "                        flag = False\n",
    "                        break\n",
    "                    temp += 1\n",
    "                \n",
    "                if flag:\n",
    "                    for k in range(len(entity_name_words)):\n",
    "                        if (k == 0):\n",
    "                            label[i] = \"B-ent\"\n",
    "                        else:\n",
    "                            label[i+k] = \"I-ent\"\n",
    "                    \n",
    "                    break\n",
    "    \n",
    "    # if(not \"B-ent\" in label):\n",
    "    #     print(\"m\",m)\n",
    "    #     print(entity_names)\n",
    "\n",
    "    curr_sample['sentence'] = curr_history\n",
    "    curr_sample[\"label\"] = label\n",
    "\n",
    "    train_data.append(curr_sample)\n",
    "\n",
    "\n",
    "with open('train.json', 'w') as f:\n",
    "    json.dump(train_data, f, indent=4)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
